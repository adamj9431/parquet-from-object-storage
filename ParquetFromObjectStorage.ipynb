{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload files to IBM Data Science Experience using the command line\n",
    "\n",
    "This notebook intended to the final step in this tutorial on Medium.\n",
    "\n",
    "Select the cell below. On the palette (right), click **Data Sources**. Choose one of the entries beginnning with \"MBTAStopFrequency.parquet\" and click **Insert to Code**. This will insert a Python dict containing Object Storage credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the dict above is named `credentials_1`. \n",
    "\n",
    "We want to access the entire Parquet table, not just one partition. Therefore, change the `filename` entry in the credentials to match the value below:\n",
    "\n",
    "`'filename':'MBTAStopFrequency.parquet/*'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_hadoop_config(credentials):\n",
    "    \"\"\"This function sets the Hadoop configuration with given credentials, \n",
    "    so it is possible to access data using SparkContext\"\"\"\n",
    "    \n",
    "    prefix = \"fs.swift.service.\" + credentials['name']\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + \".auth.url\", credentials['auth_url']+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n",
    "    hconf.set(prefix + \".tenant\", credentials['project_id'])\n",
    "    hconf.set(prefix + \".username\", credentials['user_id'])\n",
    "    hconf.set(prefix + \".password\", credentials['password'])\n",
    "    hconf.setInt(prefix + \".http.port\", 8080)\n",
    "    hconf.set(prefix + \".region\", credentials['region'])\n",
    "    hconf.setBoolean(prefix + \".public\", True)\n",
    "    \n",
    "credentials_1['name'] = 'spark'\n",
    "set_hadoop_config(credentials_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swiftURI = 'swift://' + credentials_1['container'] + \".\" + credentials_1['name'] + \"/\" + credentials_1['filename']\n",
    "mbtaStopFrequencyDF = sqlContext.read.parquet(swiftURI).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mbtaStopFrequencyDF.printSchema()\n",
    "mbtaStopFrequencyDF.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
